üìä Add Advanced Evaluation Methods: Train/Test Split & K-Fold CV (Module 2.7)

- Added new diagrams for clarity:
  ‚Ä¢ 7.5 - Estimation methods for parameters  
  ‚Ä¢ 7.6 - Example prediction with multiple linear regression  
  ‚Ä¢ 7.7 - Concerns & disadvantages of multiple linear regression  
  ‚Ä¢ 7.8   - Calculating model accuracy  
  ‚Ä¢ 7.8.3 - Accuracy types  
  ‚Ä¢ 7.8.4 - Train/Test Split approach  
  ‚Ä¢ 7.8.5 - Train/Test vs Train/Test Split comparison  
  ‚Ä¢ 7.8.6 - K-Fold Cross-Validation  

- Updated `Module2.7.txt` with new evaluation approaches:
  1Ô∏è‚É£ **Train/Test Split Approach**
      ‚Ä¢ Dataset divided into training set (e.g., rows 0‚Äì5) and testing set (e.g., rows 6‚Äì9).  
      ‚Ä¢ Model is trained on training data and evaluated on unseen test data.  
      ‚Ä¢ Provides more realistic *out-of-sample accuracy* compared to training/testing on same dataset.  
      ‚Ä¢ Limitation: results highly dependent on the specific split.  

  2Ô∏è‚É£ **K-Fold Cross-Validation**
      ‚Ä¢ Dataset is split into *k* folds (e.g., k=4).  
      ‚Ä¢ Each fold is used once for testing while remaining folds are used for training.  
      ‚Ä¢ Accuracies across folds are averaged ‚Üí reduces variance and dependency issues.  
      ‚Ä¢ Provides more consistent and reliable evaluation metric.  

- ‚ú® Key Notes:
  ‚Ä¢ Out-of-sample testing prevents data leakage into model training.  
  ‚Ä¢ Train/Test Split is simple but may cause variance in accuracy.  
  ‚Ä¢ K-Fold CV balances accuracy by averaging multiple splits.  
  ‚Ä¢ Practical foundation for building more robust evaluation pipelines.

üöÄ This commit enriches Module 2.7 with essential evaluation strategies and supporting visuals, ensuring learners understand both the *limitations* and *advantages* of each method.
